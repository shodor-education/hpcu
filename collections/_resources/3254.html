---
title: "PCI 2019 - MPI for Scalable Computing (Part 2 of 2)"
resource-url: "https://youtu.be/pVHiccTax3M?t=1045"
creator: "Giuseppe Congiu, Argonne National Laboratory, Yanfei Guo, Argonne National Laboratory, Kenneth Raffenetti, Argonne National Laboratory"
description: "The Message Passing Interface (MPI) has been the de facto standard for parallel programming for nearly two decades now, and knowledge of MPI is considered a pre-requisite for most people aiming for a career in parallel programming. This is a beginner-level tutorial aimed at introducing parallel programming with MPI. This tutorial will provide an overview of MPI, its offered features, current implementations of MPI, and its suitability for parallel computing environments. Together with a brief overview of MPI and its features the tutorial will also discuss good programming practices and issues to watch out for in MPI programming. Finally, several application case studies, including examples in nuclear physics, combustion, and quantum chemistry applications, and how they use MPI, will be shown."
relation: "https://anl.app.box.com/v/PETASCALE-MPI-2019"
rights: "Fair Use - Education"
language: "English"
audience:
  - "Learner/Student"
  - "Professional/Practitioner"
  - "Researcher"
  - "Student"
type:
  - "Event/Opportunity"
  - "Instructional Material"
  - "Lecture/Presentation"
  - "Software"
  - "Tool"
  - "Tutorial"
  - "Workshop"
subject:
  - "Chemistry"
  - "Complex Systems"
  - "Computational Science"
  - "Computer Science"
  - "Engineering"
  - "Information Technology"
format:
  - "HTML"
  - "Slide"
education-level:
  - "Graduate/Professional"
  - "Higher Education"
  - "Undergraduate (Lower Division)"
  - "Undergraduate (Upper Division)"
keyword:
  - "ANL USA"
  - "Aurora"
  - "CAS"
  - "FOP"
  - "Frontier"
  - "HMM"
  - "MPI"
  - "NUDT China"
  - "NWChem"
  - "ORNL USA"
  - "RDMA"
  - "RMA"
  - "Remote Direct Memory Access"
  - "SIMD"
  - "Single Instruction Multiple Data"
  - "Tianhe-3"
  - "UVA"
  - "Unified Virtual Addressing"
  - "accelerators"
  - "accumulate"
  - "allocate"
  - "barrier"
  - "block"
  - "calls"
  - "cast"
  - "collective computation"
  - "collectives"
  - "communicator"
  - "commutative"
  - "compare"
  - "computation"
  - "cpu"
  - "create"
  - "data level parallelism"
  - "data movement"
  - "datatype"
  - "deadlock"
  - "decouple"
  - "dynamic"
  - "epoch"
  - "fetch"
  - "flush"
  - "free"
  - "gather"
  - "get"
  - "global address space"
  - "gpu"
  - "heterogeneous memory management"
  - "intranode communication"
  - "latency"
  - "local"
  - "lock"
  - "memory model"
  - "nonblocking collectives"
  - "one-sided communication"
  - "operation"
  - "overlap"
  - "page fault"
  - "passive target synchronization"
  - "performance"
  - "pipeline"
  - "portability"
  - "pre-optimized"
  - "process"
  - "public memory"
  - "put"
  - "receive"
  - "reduce"
  - "relax"
  - "remote memory access"
  - "safety"
  - "scan"
  - "scatter"
  - "send"
  - "shared"
  - "software"
  - "stencil"
  - "swap"
  - "synchronization"
  - "test"
  - "thread"
  - "transfer"
  - "transparent"
  - "two-sided communication"
  - "type"
  - "unlock"
  - "vector"
  - "version"
  - "wait"
  - "window creation"
hpcu-subject:
  - "Architectures"
  - "Visualization"
difficulty:
  - "Beginner"
  - "Intermediate"
hpcu-subject-2:
  - "Accelerators"
  - "Accuracy and Precision"
  - "Applications"
  - "Architectures, Devices, Hardware"
  - "Attributes"
  - "Best Practices"
  - "CPU"
  - "Checkpointing"
  - "Code"
  - "Communication"
  - "Communicators"
  - "Complex Systems"
  - "Computation and Communication Overlapping"
  - "Computational Science"
  - "Computer Engineering"
  - "Computer Science"
  - "Computer Systems Organization"
  - "Computers"
  - "Computing Categories"
  - "Concurrency, Concurrent"
  - "Concurrent, Parallel, or Distributed Programming"
  - "Constrained Optimization Methods"
  - "Data Format, Representation, or Model"
  - "Data Management"
  - "Data Parallelism"
  - "Data Processing"
  - "Data Transfers"
  - "Data Types"
  - "Data, Input/Output"
  - "Deadlock"
  - "Distributed Memory"
  - "Dynamic Load Balancing"
  - "Engineering"
  - "Execution, Workload"
  - "Flow"
  - "Functional Parallelism"
  - "General Concepts"
  - "Granularity"
  - "High Performance Computing"
  - "High Throughput Computing"
  - "Information Technology"
  - "Libraries"
  - "Load Balance"
  - "Load Balancing"
  - "Load Imbalance"
  - "Many-Core Accelerators"
  - "Many-Core Computing"
  - "Memory"
  - "Memory Models and Address Space"
  - "Message Passing"
  - "Methodologies"
  - "Middleware"
  - "Mixed Data Types"
  - "Multi-Core Computing"
  - "Mutual Exclusion"
  - "Nodes"
  - "Non-Linear Programming Methods"
  - "Optimization"
  - "Parallel Architecture"
  - "Parallel Code"
  - "Parallel Data Transfers"
  - "Parallel I/O"
  - "Parallel, Parallelism"
  - "Parallelization"
  - "Partitioned Global Address Space (PGAS)"
  - "Performance"
  - "Physical Science and Engineering"
  - "Processors"
  - "Programming"
  - "Protocols"
  - "Race Condition"
  - "Resource Management"
  - "Robustness"
  - "Scalar"
  - "Scheduling"
  - "Scientific Computing"
  - "Shared Memory"
  - "Single-Instruction-Multiple-Data (SIMD)"
  - "Software"
  - "Software Categories"
  - "Software Development Stages, Lifecycle"
  - "Speedup"
  - "Subject Areas"
  - "Synchronization"
  - "Synchronization Mechanisms"
  - "Systems"
  - "Task-Level Parallelism"
  - "Vector"
hpcu-keywords:
  - "Blocking/non-Blocking Send/Receive Operations"
  - "Collective Communication"
  - "GPUs"
  - "MPI"
  - "PGAS Collective Operations"
---
